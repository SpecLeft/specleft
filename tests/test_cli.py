"""Tests for specleft.cli module."""

from __future__ import annotations

import json
import textwrap
from pathlib import Path

import pytest
from click.testing import CliRunner
from specleft.cli.main import cli
from specleft.utils.text import to_snake_case


def _write_file(path: Path, content: str) -> None:
    path.write_text(textwrap.dedent(content).strip())


def _create_feature_specs(
    base_dir: Path,
    *,
    feature_id: str,
    story_id: str,
    scenario_id: str,
    include_test_data: bool = False,
    features_dir_name: str = "features",
    scenario_priority: str = "high",
    execution_time: str = "fast",
) -> Path:
    features_dir = base_dir / features_dir_name
    feature_dir = features_dir / feature_id
    story_dir = feature_dir / story_id
    story_dir.mkdir(parents=True, exist_ok=True)

    scenario_file = story_dir / f"{scenario_id.replace('-', '_')}.md"

    _write_file(
        feature_dir / "_feature.md",
        f"""
        ---
        feature_id: {feature_id}
        priority: high
        tags: [core]
        ---

        # Feature: {feature_id.title()} Feature
        """,
    )

    _write_file(
        story_dir / "_story.md",
        f"""
        ---
        story_id: {story_id}
        tags: [smoke]
        ---

        # Story: {story_id.title()} Story
        """,
    )

    test_data_block = ""
    if include_test_data:
        test_data_block = """
        ## Test Data
        | input | expected | description |
        |-------|----------|-------------|
        | a | A | lowercase a |
        | b | B | lowercase b |
        """

    _write_file(
        scenario_file,
        f"""
        ---
        scenario_id: {scenario_id}
        priority: {scenario_priority}
        tags: [smoke]
        execution_time: {execution_time}
        ---

        # Scenario: {scenario_id.replace('-', ' ').title()}

        {test_data_block}
        ## Steps
        - **Given** a user exists
        - **When** the user logs in
        - **Then** access is granted
        """,
    )

    return features_dir


def _create_single_file_feature_spec(
    base_dir: Path,
    *,
    feature_id: str,
    scenario_id: str,
    features_dir_name: str = "features",
    scenario_priority: str = "high",
) -> Path:
    features_dir = base_dir / features_dir_name
    features_dir.mkdir(parents=True, exist_ok=True)

    _write_file(
        features_dir / f"{feature_id}.md",
        f"""
        # Feature: {feature_id.title()} Feature

        ## Scenarios

        ### Scenario: {scenario_id.replace('-', ' ').title()}
        priority: {scenario_priority}

        - Given a user exists
        - When the user logs in
        - Then access is granted
        """,
    )

    return features_dir


class TestToSnakeCase:
    """Tests for to_snake_case helper function."""

    def test_simple_hyphenated(self) -> None:
        """Test converting hyphenated string."""
        assert to_snake_case("login-success") == "login_success"

    def test_already_snake_case(self) -> None:
        """Test string that's already snake_case."""
        assert to_snake_case("login_success") == "login_success"

    def test_camel_case(self) -> None:
        """Test converting camelCase."""
        assert to_snake_case("loginSuccess") == "login_success"

    def test_with_spaces(self) -> None:
        """Test converting string with spaces."""
        assert to_snake_case("login success") == "login_success"

    def test_mixed_format(self) -> None:
        """Test converting mixed format string."""
        assert to_snake_case("Login-Success Test") == "login_success_test"

    def test_multiple_hyphens(self) -> None:
        """Test handling multiple consecutive hyphens."""
        assert to_snake_case("login--success") == "login_success"


class TestCLIBase:
    """Tests for CLI base commands."""

    def test_cli_version(self) -> None:
        """Test --version flag."""
        runner = CliRunner()
        result = runner.invoke(cli, ["--version"])
        assert result.exit_code == 0
        assert "0.2.0" in result.output

    def test_cli_help(self) -> None:
        """Test --help flag."""
        runner = CliRunner()
        result = runner.invoke(cli, ["--help"])
        assert result.exit_code == 0
        assert "SpecLeft" in result.output
        assert "test" in result.output
        assert "features" in result.output
        assert "contract" in result.output


class TestContractCommand:
    """Tests for 'specleft contract' commands."""

    def test_contract_json_output(self) -> None:
        runner = CliRunner()
        result = runner.invoke(cli, ["contract", "--format", "json"])
        assert result.exit_code == 0
        payload = json.loads(result.output)
        assert payload["contract_version"] == "1.0"
        assert payload["specleft_version"] == "0.2.0"
        assert "guarantees" in payload

    def test_contract_test_json_output(self) -> None:
        runner = CliRunner()
        result = runner.invoke(cli, ["contract", "test", "--format", "json"])
        assert result.exit_code == 0
        payload = json.loads(result.output)
        assert payload["contract_version"] == "1.0"
        assert payload["specleft_version"] == "0.2.0"
        assert payload["passed"] is True
        assert payload["checks"]


class TestTestGroup:
    """Tests for 'test' command group."""

    def test_test_group_help(self) -> None:
        """Test 'test' group help."""
        runner = CliRunner()
        result = runner.invoke(cli, ["test", "--help"])
        assert result.exit_code == 0
        assert "skeleton" in result.output
        assert "report" in result.output


class TestSkeletonCommand:
    """Tests for 'specleft test skeleton' command."""

    def test_skeleton_missing_features_dir(self) -> None:
        """Test skeleton command when features directory is missing."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["test", "skeleton"])
            assert result.exit_code == 1
            assert "not found" in result.output

    def test_skeleton_generates_single_file(self) -> None:
        """Test skeleton command with --single-file flag."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )

            result = runner.invoke(
                cli, ["test", "skeleton", "--single-file"], input="y\n"
            )
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output

            generated_file = Path("tests/test_generated.py")
            assert generated_file.exists()

            content = generated_file.read_text()
            assert "from specleft import specleft" in content
            assert 'feature_id="auth"' in content
            assert 'scenario_id="login-success"' in content
            assert "skip=True" in content
            assert "Skeleton test - not yet implemented" in content
            assert "def test_login_success" in content
            assert "with specleft.step('Given a user exists'):" in content
            assert "with specleft.step('When the user logs in'):" in content
            assert "with specleft.step('Then access is granted'):" in content
            assert "assert not True" not in content
            assert "Preview:" in result.output

    def test_skeleton_auto_detects_single_file_layout(self) -> None:
        """Test skeleton command auto-detects single-file layout."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="payments",
                scenario_id="card-charge",
            )

            result = runner.invoke(cli, ["test", "skeleton"], input="y\n")
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output

            generated_file = Path("tests/test_payments.py")
            assert generated_file.exists()

            content = generated_file.read_text()
            assert "from specleft import specleft" in content
            assert 'feature_id="payments"' in content
            assert 'scenario_id="card-charge"' in content

    def test_skeleton_auto_detects_nested_layout(self) -> None:
        """Test skeleton command auto-detects nested layout."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="legacy",
                story_id="billing",
                scenario_id="legacy-charge",
            )

            result = runner.invoke(cli, ["test", "skeleton"], input="y\n")
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output

            generated_file = Path("tests/legacy/test_billing.py")
            assert generated_file.exists()

    def test_skeleton_custom_output_dir(self) -> None:
        """Test skeleton command with custom output directory (single-file layout)."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="inventory",
                scenario_id="check-stock",
            )

            result = runner.invoke(
                cli, ["test", "skeleton", "--output-dir", "custom_tests"], input="y\n"
            )
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output

            generated_file = Path("custom_tests/test_inventory.py")
            assert generated_file.exists()

    def test_skeleton_custom_features_dir(self) -> None:
        """Test skeleton command with custom features directory (single-file layout)."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            features_dir = _create_single_file_feature_spec(
                Path("."),
                feature_id="support",
                scenario_id="open-ticket",
                features_dir_name="specs",
            )

            result = runner.invoke(
                cli, ["test", "skeleton", "-f", str(features_dir)], input="y\n"
            )
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output
            assert Path("tests/test_support.py").exists()

    def test_skeleton_with_parameterized_tests(self) -> None:
        """Test skeleton command generates parameterized tests correctly."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="params",
                story_id="format",
                scenario_id="formatting",
                include_test_data=True,
            )

            result = runner.invoke(
                cli, ["test", "skeleton", "--single-file"], input="y\n"
            )
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output

            content = Path("tests/test_generated.py").read_text()
            assert "@pytest.mark.parametrize" in content
            assert "input, expected" in content
            assert "'a'" in content
            assert "'A'" in content
            assert "skip=True" in content

    def test_skeleton_invalid_dir(self) -> None:
        """Test skeleton command with invalid features directory."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("features").mkdir()

            result = runner.invoke(cli, ["test", "skeleton"])
            assert result.exit_code == 0
            assert "No specs found" in result.output

    def test_skeleton_invalid_schema(self) -> None:
        """Test skeleton command with invalid spec schema."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            features_dir = Path("features")
            features_dir.mkdir()
            feature_dir = features_dir / "invalid"
            feature_dir.mkdir()
            (feature_dir / "_feature.md").write_text("---\nfeature_id: INVALID\n---")

            result = runner.invoke(cli, ["test", "skeleton"])
            assert result.exit_code == 1
            assert "Error loading" in result.output

    def test_skeleton_shows_next_steps(self) -> None:
        """Test skeleton command shows next steps."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["test", "skeleton"], input="n\n")
            assert result.exit_code == 2
            assert "Confirm creation?" in result.output
            assert "Cancelled" in result.output
            assert not Path("tests/test_auth.py").exists()

    def test_skeleton_preview_output(self) -> None:
        """Test skeleton command outputs a preview."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["test", "skeleton"], input="n\n")
            assert result.exit_code == 2
            assert "File: tests/test_auth.py" in result.output
            assert "Scenario IDs: login-success" in result.output
            assert "Steps (first scenario): 3" in result.output
            assert "Status: SKIPPED (not implemented)" in result.output
            assert "Preview:" in result.output
            assert "Confirm creation?" in result.output
            assert "Cancelled" in result.output

    def test_skeleton_skips_existing_file(self) -> None:
        """Test skeleton command skips existing files."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )

            first_run = runner.invoke(cli, ["test", "skeleton"], input="y\n")
            assert first_run.exit_code == 0
            generated_file = Path("tests/test_auth.py")
            assert generated_file.exists()
            initial_content = generated_file.read_text()

            second_run = runner.invoke(cli, ["test", "skeleton"], input="y\n")
            assert second_run.exit_code == 0
            assert "No new skeleton tests to generate." in second_run.output
            assert generated_file.read_text() == initial_content

    def test_skeleton_tests_are_skipped_in_pytest(self) -> None:
        """Integration test: verify skeleton tests run as SKIPPED in pytest."""
        import subprocess

        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )

            # Generate skeleton test
            result = runner.invoke(cli, ["test", "skeleton"], input="y\n")
            assert result.exit_code == 0
            assert "Confirm creation?" in result.output
            assert "✓ Created 1 test files" in result.output
            generated_file = Path("tests/test_auth.py")
            assert generated_file.exists()

            # Run pytest on the generated skeleton
            pytest_result = subprocess.run(
                ["pytest", str(generated_file), "-v"],
                capture_output=True,
                text=True,
                timeout=30,
            )

            # Verify the test was SKIPPED, not failed
            output = pytest_result.stdout + pytest_result.stderr
            assert "SKIPPED" in output or "skipped" in output.lower()
            assert "FAILED" not in output
            assert "FAILED" not in output
            # Pytest exit code 0 means all tests passed/skipped (no failures)
            assert pytest_result.returncode == 0
            # Check for the skip reason (may be truncated in verbose output)
            assert "Skeleton test" in output


class TestFeaturesGroup:
    """Tests for 'features' command group."""

    def test_features_group_help(self) -> None:
        """Test 'features' group help."""
        runner = CliRunner()
        result = runner.invoke(cli, ["features", "--help"])
        assert result.exit_code == 0
        assert "validate" in result.output
        assert "list" in result.output
        assert "stats" in result.output


class TestValidateCommand:
    """Tests for 'specleft features validate' command."""

    def test_validate_valid_dir(self) -> None:
        """Test validate command with valid specs directory."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["features", "validate"])
            assert result.exit_code == 0
            assert "is valid" in result.output
            assert "Features: 1" in result.output
            assert "Stories: 1" in result.output
            assert "Detected nested feature structure" in result.output
            assert "Scenarios: 1" in result.output
            assert "Steps: 3" in result.output

    def test_validate_json_output(self) -> None:
        """Test validate command JSON output."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["features", "validate", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["valid"] is True
            assert payload["features"] == 1

    def test_validate_missing_dir(self) -> None:
        """Test validate command when directory is missing."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["features", "validate"])
            assert result.exit_code == 1
            assert "Directory not found" in result.output

    def test_validate_invalid_dir(self) -> None:
        """Test validate command with invalid specs directory."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("features").mkdir()

            result = runner.invoke(cli, ["features", "validate"])
            assert result.exit_code == 1
            assert "Validation failed" in result.output

    def test_validate_custom_dir(self) -> None:
        """Test validate command with custom directory."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            features_dir = _create_feature_specs(
                Path("."),
                feature_id="payments",
                story_id="refunds",
                scenario_id="refund",
                features_dir_name="specs",
            )

            result = runner.invoke(
                cli, ["features", "validate", "--dir", str(features_dir)]
            )
            assert result.exit_code == 0
            assert "is valid" in result.output
            assert "Detected nested feature structure" in result.output


class TestFeaturesListCommand:
    """Tests for 'specleft features list' command."""

    def test_features_list_outputs_tree(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["features", "list"])
            assert result.exit_code == 0
            assert "Features (1):" in result.output
            assert "- auth: Auth Feature" in result.output
            assert "- login: Login Story" in result.output
            assert "- login-success: Login Success" in result.output
            assert "Detected nested feature structure" in result.output

    def test_features_list_json_output(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            result = runner.invoke(cli, ["features", "list", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["summary"]["features"] == 1
            feature = payload["features"][0]
            assert feature["feature_id"] == "auth"
            # Canonical feature shape with flattened scenarios
            assert feature["title"] == "Auth Feature"
            assert "confidence" in feature  # nullable
            assert "scenarios" in feature  # flattened from stories
            assert "stories" not in feature  # no stories nesting
            scenario = feature["scenarios"][0]
            assert scenario["id"] == "login-success"  # canonical uses 'id'

    def test_nested_structure_warning(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="legacy",
                story_id="billing",
                scenario_id="legacy-charge",
            )

            result = runner.invoke(cli, ["features", "list"])
            assert result.exit_code == 0
            assert "Detected nested feature structure" in result.output
            assert "features/legacy/_feature.md" not in result.output

    def test_features_list_missing_dir(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["features", "list"])
            assert result.exit_code == 1
            assert "Directory not found" in result.output


class TestPlanCommand:
    """Tests for 'specleft plan' command."""

    def test_plan_missing_prd_warns(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["plan"])
            assert result.exit_code == 0
            assert "PRD not found" in result.output
            assert "Expected locations" in result.output

    def test_plan_creates_features_from_h2(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("# PRD\n\n## User Authentication\n## Payments\n")
            result = runner.invoke(cli, ["plan"])
            assert result.exit_code == 0
            assert Path("features/user-authentication.md").exists()
            assert Path("features/payments.md").exists()
            assert "Features planned: 2" in result.output

            content = Path("features/user-authentication.md").read_text()
            assert "# Feature: User Authentication" in content
            assert "### Scenario: Example" in content

    def test_plan_uses_h1_when_no_h2(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("# User Authentication\n")
            result = runner.invoke(cli, ["plan"])
            assert result.exit_code == 0
            assert Path("features/user-authentication.md").exists()
            assert "using top-level title" in result.output

    def test_plan_defaults_to_prd_file_when_no_headings(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("No headings here")
            result = runner.invoke(cli, ["plan"])
            assert result.exit_code == 0
            assert Path("features/prd.md").exists()
            assert "creating features/prd.md" in result.output

    def test_plan_dry_run_creates_nothing(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("# User Authentication\n")
            result = runner.invoke(cli, ["plan", "--dry-run"])
            assert result.exit_code == 0
            assert not Path("features").exists()
            assert "Dry run" in result.output
            assert "Would create:" in result.output

    def test_plan_json_output(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("# User Authentication\n")
            result = runner.invoke(cli, ["plan", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["feature_count"] == 1
            assert payload["created"]

    def test_plan_json_dry_run(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("prd.md").write_text("# User Authentication\n")
            result = runner.invoke(cli, ["plan", "--format", "json", "--dry-run"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["dry_run"] is True
            assert payload["would_create"]
            assert "created" not in payload

    def test_plan_skips_existing_feature(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            features_dir = Path("features")
            features_dir.mkdir()
            feature_file = features_dir / "user-authentication.md"
            feature_file.write_text("# Feature: User Authentication\n")

            Path("prd.md").write_text("# User Authentication\n")
            result = runner.invoke(cli, ["plan"])
            assert result.exit_code == 0
            assert "Skipped existing" in result.output
            assert feature_file.read_text() == "# Feature: User Authentication\n"


class TestDoctorCommand:
    """Tests for 'specleft doctor' command."""

    def test_doctor_json_includes_version(self) -> None:
        runner = CliRunner()
        result = runner.invoke(cli, ["doctor", "--format", "json"])
        assert result.exit_code in {0, 1}
        payload = json.loads(result.output)
        assert payload["version"] == "0.2.0"
        assert "healthy" in payload
        assert "checks" in payload

    def test_doctor_table_output(self) -> None:
        runner = CliRunner()
        result = runner.invoke(cli, ["doctor"])
        assert result.exit_code in {0, 1}
        assert "Checking SpecLeft installation" in result.output
        assert "specleft CLI available" in result.output

    def test_doctor_verbose_output(self) -> None:
        runner = CliRunner()
        result = runner.invoke(cli, ["doctor", "--verbose"])
        assert result.exit_code in {0, 1}
        assert "pytest plugin" in result.output


class TestStatusCommand:
    """Tests for 'specleft status' command."""

    def test_status_json_includes_execution_time(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                execution_time="slow",
            )
            result = runner.invoke(cli, ["status", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            scenarios = payload["features"][0]["scenarios"]
            assert scenarios[0]["execution_time"] == "slow"

    def test_status_groups_by_feature_file(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_single_file_feature_spec(
                Path("."),
                feature_id="auth",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["status"])
            assert result.exit_code == 0
            assert "Feature File: features/auth.md" in result.output
            assert "login-success" in result.output

    def test_status_unimplemented_table_output(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["status", "--unimplemented"])
            assert result.exit_code == 0
            assert "Unimplemented Scenarios" in result.output
            assert "⚠ auth/login/login-success" in result.output
            assert "Detected nested feature structure" in result.output

    def test_status_filter_requires_valid_ids(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["status", "--feature", "missing"])
            assert result.exit_code == 1
            assert "Unknown feature ID" in result.output

    def test_status_treats_missing_test_file_as_skipped(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["status", "--format", "json"])
            payload = json.loads(result.output)
            scenario = payload["features"][0]["scenarios"][0]
            assert scenario["status"] == "skipped"
            assert "test_login.py" in scenario["test_file"]  # nested layout default

    def test_status_marks_skipped_when_decorator_skip_true(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            tests_dir = Path("tests") / "auth"
            tests_dir.mkdir(parents=True)
            (tests_dir / "test_login.py").write_text(
                """
from specleft import specleft

@specleft(feature_id="auth", scenario_id="login-success", skip=True)
def test_login_success():
    pass
"""
            )

            result = runner.invoke(cli, ["status", "--format", "json"])
            scenario = json.loads(result.output)["features"][0]["scenarios"][0]
            assert scenario["status"] == "skipped"
            assert scenario["reason"] == "Not implemented"

    def test_status_marks_implemented_when_skip_removed(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            tests_dir = Path("tests") / "auth"
            tests_dir.mkdir(parents=True)
            (tests_dir / "test_login.py").write_text(
                """
from specleft import specleft

@specleft(feature_id="auth", scenario_id="login-success")
def test_login_success():
    pass
"""
            )

            result = runner.invoke(cli, ["status", "--format", "json"])
            scenario = json.loads(result.output)["features"][0]["scenarios"][0]
            assert scenario["status"] == "implemented"

    def test_status_json_canonical_shape(self) -> None:
        """Test status JSON output matches canonical feature shape."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["status", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)

            feature = payload["features"][0]
            # Canonical feature fields
            assert feature["feature_id"] == "auth"
            assert feature["title"] == "Auth Feature"
            assert "confidence" in feature  # nullable
            assert "source" in feature  # nullable
            assert "assumptions" in feature  # nullable
            assert "open_questions" in feature  # nullable
            assert "owner" in feature  # nullable
            assert "component" in feature  # nullable
            assert "tags" in feature

            # Scenarios flattened (no stories nesting)
            assert "scenarios" in feature
            assert "stories" not in feature
            scenario = feature["scenarios"][0]
            assert (
                scenario["id"] == "login-success"
            )  # canonical uses 'id' not 'scenario_id'
            assert scenario["title"] == "Login Success"
            assert "priority" in scenario
            assert "tags" in scenario
            assert "steps" in scenario


class TestNextCommand:
    """Tests for 'specleft next' command."""

    def test_next_json_output(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            result = runner.invoke(cli, ["next", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["showing"] == 1

    def test_next_json_omits_execution_time(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                execution_time="slow",
            )
            result = runner.invoke(cli, ["next", "--format", "json"])
            payload = json.loads(result.output)
            scenario = payload["tests"][0]
            assert "execution_time" not in scenario

    def test_next_respects_priority_and_limit(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                scenario_priority="low",
            )
            _create_feature_specs(
                Path("."),
                feature_id="payments",
                story_id="refunds",
                scenario_id="issue-refund",
                scenario_priority="critical",
            )
            result = runner.invoke(cli, ["next", "--format", "json", "--limit", "1"])
            payload = json.loads(result.output)
            assert payload["showing"] == 1
            assert payload["tests"][0]["scenario_id"] == "issue-refund"

    def test_next_filters_by_priority(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                scenario_priority="high",
            )
            _create_feature_specs(
                Path("."),
                feature_id="payments",
                story_id="refunds",
                scenario_id="issue-refund",
                scenario_priority="low",
            )
            result = runner.invoke(
                cli, ["next", "--format", "json", "--priority", "high"]
            )
            payload = json.loads(result.output)
            assert payload["showing"] == 1
            assert payload["tests"][0]["scenario_id"] == "login-success"

    def test_next_outputs_success_when_all_implemented(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )
            tests_dir = Path("tests") / "auth"
            tests_dir.mkdir(parents=True)
            (tests_dir / "test_login.py").write_text(
                """
from specleft import specleft

@specleft(feature_id="auth", scenario_id="login-success")
def test_login_success():
    pass
"""
            )
            result = runner.invoke(cli, ["next"])
            assert result.exit_code == 0
            assert "All scenarios are implemented" in result.output


class TestFeaturesStatsCommand:
    """Tests for 'specleft features stats' command."""

    def test_features_stats_outputs_summary(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                include_test_data=True,
            )

            result = runner.invoke(cli, ["features", "stats"])
            assert result.exit_code == 0
            # New format includes test coverage stats
            assert "Test Coverage Stats" in result.output
            assert "Pytest Tests:" in result.output
            assert "Total pytest tests discovered:" in result.output
            assert "Tests with @specleft:" in result.output
            # Specifications section
            assert "Specifications:" in result.output
            assert "Features: 1" in result.output
            assert "Stories: 1" in result.output
            assert "Scenarios: 1" in result.output
            assert "Steps: 3" in result.output
            assert "Parameterized scenarios: 1" in result.output
            assert "Tags:" in result.output
            # Coverage section
            assert "Coverage:" in result.output
            assert "Scenarios with tests:" in result.output
            assert "Scenarios without tests:" in result.output
            assert "Detected nested feature structure" in result.output

    def test_features_stats_json_output(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
                include_test_data=True,
            )

            result = runner.invoke(cli, ["features", "stats", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["specs"]["features"] == 1
            assert payload["coverage"]["scenarios_without_tests"] >= 0

    def test_features_stats_missing_dir(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["features", "stats"])
            assert result.exit_code == 1
            assert "Directory not found" in result.output

    def test_features_stats_with_matching_tests(self) -> None:
        """Test stats when there are tests that match specs."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            # Create spec
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            # Create a test file with @specleft decorator matching the spec
            tests_dir = Path("tests")
            tests_dir.mkdir()
            test_file = tests_dir / "test_auth.py"
            test_file.write_text(
                """
from specleft import specleft

@specleft(feature_id="auth", scenario_id="login-success")
def test_login_success():
    pass
"""
            )

            result = runner.invoke(cli, ["features", "stats"])
            assert result.exit_code == 0
            assert "Scenarios with tests: 1" in result.output
            assert "Scenarios without tests: 0" in result.output
            assert "Coverage: 100.0%" in result.output
            assert "Detected nested feature structure" in result.output

    def test_features_stats_with_partial_coverage(self) -> None:
        """Test stats when some scenarios have tests and some don't."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            # Use the helper to create proper spec structure
            _create_feature_specs(
                Path("."),
                feature_id="auth",
                story_id="login",
                scenario_id="login-success",
            )

            # Add a second scenario manually
            scenario_dir = Path("features/auth/login")
            (scenario_dir / "login-failure.md").write_text(
                """---
scenario_id: login-failure
name: Login Failure
priority: high
---
## Scenario: Login Failure
Given a user exists
When the user logs in with wrong password
Then access is denied
"""
            )

            # Create test file with only one @specleft test
            tests_dir = Path("tests")
            tests_dir.mkdir()
            test_file = tests_dir / "test_auth.py"
            test_file.write_text(
                """
from specleft import specleft

@specleft(feature_id="auth", scenario_id="login-success")
def test_login_success():
    pass
"""
            )

            result = runner.invoke(cli, ["features", "stats"])
            assert result.exit_code == 0
            assert "Scenarios: 2" in result.output
            assert "Scenarios with tests: 1" in result.output
            assert "Scenarios without tests: 1" in result.output
            assert "Coverage: 50.0%" in result.output
            # Should list the uncovered scenario
            assert "login-failure" in result.output
            assert "Detected nested feature structure" in result.output


@pytest.fixture
def sample_results() -> dict:
    """Sample results JSON for testing."""
    return {
        "run_id": "2025-01-13T10:00:00",
        "summary": {
            "total_features": 1,
            "total_scenarios": 2,
            "total_executions": 3,
            "passed": 2,
            "failed": 1,
            "skipped": 0,
            "duration": 1.234,
        },
        "features": [
            {
                "feature_id": "TEST-001",
                "feature_name": "Test Feature",
                "scenarios": [
                    {
                        "scenario_id": "scenario-one",
                        "scenario_name": "Scenario One",
                        "is_parameterized": False,
                        "executions": [
                            {
                                "test_name": "test_scenario_one",
                                "status": "passed",
                                "duration": 0.5,
                                "steps": [
                                    {
                                        "description": "Given something",
                                        "status": "passed",
                                        "duration": 0.1,
                                    }
                                ],
                            },
                        ],
                        "summary": {
                            "total": 1,
                            "passed": 1,
                            "failed": 0,
                            "skipped": 0,
                        },
                    },
                ],
            },
        ],
    }


class TestReportCommand:
    """Tests for 'specleft test report' command."""

    def test_report_json_output(self, sample_results: dict) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            results_dir = Path(".specleft/results")
            results_dir.mkdir(parents=True)
            results_file = results_dir / "results_20250113_100000.json"
            results_file.write_text(json.dumps(sample_results))

            result = runner.invoke(cli, ["test", "report", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["status"] == "ok"
            assert payload["summary"]["total_features"] == 1

    def test_init_creates_single_file_example(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["init"], input="1\n")
            assert result.exit_code == 0
            assert Path("features/example-feature.md").exists()

    def test_init_json_dry_run(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["init", "--dry-run", "--format", "json"])
            assert result.exit_code == 0
            payload = json.loads(result.output)
            assert payload["dry_run"] is True
            assert payload["summary"]["directories"] == 3
            assert "features/example-feature.md" in payload["would_create"]

    def test_init_json_requires_dry_run(self) -> None:
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["init", "--format", "json"])
            assert result.exit_code == 1
            payload = json.loads(result.output)
            assert payload["status"] == "error"

    def test_report_no_results(self) -> None:
        """Test report command when no results exist."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["test", "report"])
            assert result.exit_code == 1
            assert "No results found" in result.output

    def test_report_generates_html(self, sample_results: dict) -> None:
        """Test report command generates HTML file."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            results_dir = Path(".specleft/results")
            results_dir.mkdir(parents=True)
            results_file = results_dir / "results_20250113_100000.json"
            results_file.write_text(json.dumps(sample_results))

            result = runner.invoke(cli, ["test", "report"])
            assert result.exit_code == 0
            assert "Report generated" in result.output

            assert Path("report.html").exists()

            content = Path("report.html").read_text()
            assert "SpecLeft Test Report" in content
            assert "TEST-001" in content
            assert "passed" in content.lower()

    def test_report_custom_output(self, sample_results: dict) -> None:
        """Test report command with custom output path."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            results_dir = Path(".specleft/results")
            results_dir.mkdir(parents=True)
            results_file = results_dir / "results_20250113_100000.json"
            results_file.write_text(json.dumps(sample_results))

            result = runner.invoke(cli, ["test", "report", "-o", "custom_report.html"])
            assert result.exit_code == 0
            assert Path("custom_report.html").exists()

    def test_report_specific_results_file(self, sample_results: dict) -> None:
        """Test report command with specific results file."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("my_results.json").write_text(json.dumps(sample_results))

            result = runner.invoke(cli, ["test", "report", "-r", "my_results.json"])
            assert result.exit_code == 0
            assert Path("report.html").exists()

    def test_report_missing_specific_file(self) -> None:
        """Test report command with non-existent specific results file."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            result = runner.invoke(cli, ["test", "report", "-r", "nonexistent.json"])
            assert result.exit_code == 1
            assert "not found" in result.output

    def test_report_invalid_json(self) -> None:
        """Test report command with invalid JSON results."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            Path("results.json").write_text("invalid json")

            result = runner.invoke(cli, ["test", "report", "-r", "results.json"])
            assert result.exit_code == 1
            assert "Invalid JSON" in result.output

    def test_report_uses_latest_file(self, sample_results: dict) -> None:
        """Test report command uses the latest results file when multiple exist."""
        runner = CliRunner()
        with runner.isolated_filesystem():
            results_dir = Path(".specleft/results")
            results_dir.mkdir(parents=True)

            (results_dir / "results_20250101_100000.json").write_text(
                json.dumps({**sample_results, "run_id": "old-run"})
            )

            (results_dir / "results_20250113_100000.json").write_text(
                json.dumps({**sample_results, "run_id": "new-run"})
            )

            result = runner.invoke(cli, ["test", "report"])
            assert result.exit_code == 0

            content = Path("report.html").read_text()
            assert "new-run" in content
