"""
Acceptance tests for Feature 4: Status & Coverage Inspection.

These tests verify that users and agents can inspect implementation
progress without enforcing behaviour.

Generated by SpecLeft - https://github.com/SpecLeft/specleft
"""

from __future__ import annotations

import json
from pathlib import Path

from click.testing import CliRunner

from specleft import specleft
from specleft.cli.main import cli
from conftest import FeatureFiles

# =============================================================================
# Feature: Feature 4: Status & Coverage Inspection
# ID: feature-4-status-coverage-inspection
# priority: high
# =============================================================================

# Story: Default
# ID: default


@specleft(
    feature_id="feature-4-status-coverage-inspection",
    scenario_id="report-unimplemented-scenarios",
)
def test_report_unimplemented_scenarios(
    feature_4_unimplemented: tuple[CliRunner, Path, FeatureFiles],
) -> None:
    """Report unimplemented scenarios

    Priority: high

    Verifies that `specleft status --unimplemented` filters output
    to show only scenarios that are not yet implemented.
    """
    runner, _workspace, _files = feature_4_unimplemented

    with specleft.step("Given feature scenarios exist"):
        # Feature file is already created by fixture
        pass

    with specleft.step("And some scenarios are not implemented"):
        # Test file is already created by fixture with partial implementation
        pass

    with specleft.step(
        "When specleft status --unimplemented --format json is executed"
    ):
        result = runner.invoke(cli, ["status", "--unimplemented", "--format", "json"])
        assert result.exit_code == 0, f"Command failed: {result.output}"
        payload = json.loads(result.output)

    with specleft.step("Then unimplemented scenarios are reported clearly"):
        # Should have features in output
        assert "features" in payload, "Expected 'features' key in JSON output"
        assert len(payload["features"]) >= 1, "Expected at least one feature"

        feature = payload["features"][0]
        scenarios = feature["scenarios"]

        # Should only contain unimplemented (skipped) scenarios
        scenario_ids = {s["id"] for s in scenarios}

        # user-logs-in-successfully should NOT be in the list (it's implemented)
        assert (
            "user-logs-in-successfully" not in scenario_ids
        ), "Implemented scenario should not appear in --unimplemented output"

        # The other two should be present
        assert (
            "user-password-reset" in scenario_ids
        ), "Unimplemented scenario 'user-password-reset' should be reported"
        assert (
            "user-logout" in scenario_ids
        ), "Unimplemented scenario 'user-logout' should be reported"

        # Verify status field indicates skipped
        for scenario in scenarios:
            assert (
                scenario["status"] == "skipped"
            ), f"Expected status 'skipped' for {scenario['id']}, got '{scenario['status']}'"

        # Verify summary reflects filtered results
        assert "summary" in payload, "Expected 'summary' in output"
        assert payload["summary"]["skipped"] == len(
            scenarios
        ), "Summary skipped count should match filtered scenarios"


@specleft(
    feature_id="feature-4-status-coverage-inspection",
    scenario_id="report-implemented-scenarios",
)
def test_report_implemented_scenarios(
    feature_4_implemented: tuple[CliRunner, Path, FeatureFiles],
) -> None:
    """Report implemented scenarios

    Priority: high

    Verifies that `specleft status --implemented` filters output
    to show only scenarios that are implemented.
    """
    runner, _workspace, _files = feature_4_implemented

    with specleft.step("Given feature scenarios exist"):
        # Feature file is already created by fixture
        pass

    with specleft.step("And some scenarios are implemented"):
        # Test file is already created by fixture with partial implementation
        pass

    with specleft.step("When specleft status --implemented --format json is executed"):
        result = runner.invoke(cli, ["status", "--implemented", "--format", "json"])
        assert result.exit_code == 0, f"Command failed: {result.output}"
        payload = json.loads(result.output)

    with specleft.step("Then implemented scenarios are reported clearly"):
        # Should have features in output
        assert "features" in payload, "Expected 'features' key in JSON output"
        assert len(payload["features"]) >= 1, "Expected at least one feature"

        feature = payload["features"][0]
        scenarios = feature["scenarios"]

        # Should only contain implemented scenarios
        scenario_ids = {s["id"] for s in scenarios}

        assert (
            "process-credit-card-payment" in scenario_ids
        ), "Implemented scenario should be reported"
        assert (
            "process-refund" in scenario_ids
        ), "Implemented scenario should be reported"
        assert (
            "payment-history" not in scenario_ids
        ), "Unimplemented scenario should not appear in --implemented output"

        # Verify status field indicates implemented
        for scenario in scenarios:
            assert (
                scenario["status"] == "implemented"
            ), f"Expected status 'implemented' for {scenario['id']}, got '{scenario['status']}'"

        # Verify summary reflects filtered results
        assert "summary" in payload, "Expected 'summary' in output"
        assert payload["summary"]["implemented"] == len(
            scenarios
        ), "Summary implemented count should match filtered scenarios"


@specleft(
    feature_id="feature-4-status-coverage-inspection",
    scenario_id="status-of-implementation-by-feature",
)
def test_status_of_implementation_by_feature(
    feature_4_multi_feature_filter: tuple[CliRunner, Path, FeatureFiles, FeatureFiles],
) -> None:
    """Status of implementation by feature

    Priority: medium

    Verifies that `specleft status --feature <id>` filters output
    to show only scenarios for that specific feature with a summary.
    """
    runner, _workspace, _auth_files, _billing_files = feature_4_multi_feature_filter

    with specleft.step("Given a scenario title exists"):
        # Feature files are already created by fixture
        pass

    with specleft.step("And test has not been implemented"):
        # Test files are already created by fixture
        pass

    with specleft.step(
        "When command is run spec status --feature feature-billing --format json"
    ):
        result = runner.invoke(
            cli, ["status", "--feature", "feature-billing", "--format", "json"]
        )
        assert result.exit_code == 0, f"Command failed: {result.output}"
        payload = json.loads(result.output)

    with specleft.step(
        "Then feature summary is given for the scenarios and it's status"
    ):
        # Should only have the billing feature
        assert "features" in payload, "Expected 'features' key in JSON output"
        assert (
            len(payload["features"]) == 1
        ), f"Expected exactly 1 feature (billing), got {len(payload['features'])}"

        feature = payload["features"][0]

        # Verify it's the billing feature
        assert (
            feature["feature_id"] == "feature-billing"
        ), f"Expected feature-billing, got {feature['feature_id']}"

        # Verify scenarios are present
        scenarios = feature["scenarios"]
        assert len(scenarios) == 2, f"Expected 2 scenarios, got {len(scenarios)}"

        scenario_ids = {s["id"] for s in scenarios}
        assert "generate-invoice" in scenario_ids
        assert "apply-discount" in scenario_ids

        # Verify each scenario has status
        for scenario in scenarios:
            assert (
                "status" in scenario
            ), f"Missing 'status' for scenario {scenario['id']}"
            # Both should be skipped since no tests exist
            assert (
                scenario["status"] == "skipped"
            ), f"Expected 'skipped' for {scenario['id']}, got '{scenario['status']}'"

        # Verify feature-level summary
        assert "summary" in feature, "Expected 'summary' in feature"
        feature_summary = feature["summary"]
        assert "total_scenarios" in feature_summary
        assert "implemented" in feature_summary
        assert "skipped" in feature_summary
        assert feature_summary["total_scenarios"] == 2
        assert feature_summary["implemented"] == 0
        assert feature_summary["skipped"] == 2

        # Verify coverage_percent at feature level
        assert "coverage_percent" in feature, "Expected 'coverage_percent' in feature"
        assert (
            feature["coverage_percent"] == 0
        ), f"Expected 0% coverage, got {feature['coverage_percent']}%"

        # Verify overall summary only counts the filtered feature
        assert "summary" in payload, "Expected 'summary' in payload"
        overall = payload["summary"]
        assert overall["total_features"] == 1, "Should only count filtered feature"
        assert (
            overall["total_scenarios"] == 2
        ), "Should only count scenarios from filtered feature"
